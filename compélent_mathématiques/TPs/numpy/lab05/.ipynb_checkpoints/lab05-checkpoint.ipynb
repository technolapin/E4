{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize OK\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('lab05.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Determine the step-size\n",
    "\n",
    "This assignment is composed of 10 exercises. For each solved exercise, you get the points indicated below. You need to score at least **7 points** (out of 9) to pass the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "\n",
    " - Download a copy of this notebook from [Blackboard](https://esiee.blackboard.com/).\n",
    " \n",
    " \n",
    " - Run `jupyter notebook` on your computer, and open the `.ipynb` file that you just downloaded.\n",
    "\n",
    "\n",
    " - Solve the quizzes by filling in the cells with your solutions. \n",
    " \n",
    " \n",
    " - Check your answer by running the unit test provided at the end of each quiz.\n",
    " \n",
    " \n",
    " - **Get your work checked before leaving the lab, otherwise you won't get any credit for it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "| Exercise | Topic | Points |\n",
    "|----------|------|--------|\n",
    "| Warm up | Gradient descent | 0 |\n",
    "| Quiz 1.1 | Linear model | 1 |\n",
    "| Quiz 1.2 | MSE cost | 1 |\n",
    "| Quiz 1.3 | Linear regression | 1 |\n",
    "| Quiz 2.1 | Tanh model | 1 |\n",
    "| Quiz 2.2 | Tanh regression | 1 |\n",
    "| Quiz 2.3 | ReLU model | 1 |\n",
    "| Quiz 2.4 | ReLU regression | 1 |\n",
    "| Quiz 3.1 | Lipschitz constant | 1 |\n",
    "| Quiz 3.2 | Poly regression | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages\n",
    "\n",
    "For this assignment, you need to import the following packages.\n",
    "- [**Numpy**](www.numpy.org) - The library for scientific computing in Python.\n",
    "- [**Matplotlib**](http://matplotlib.org) - The library for plotting graphs in Python.\n",
    "- [**Autograd**](https://github.com/HIPS/autograd) - The library for automatic differentiation of Numpy code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up\n",
    "\n",
    "> **Complete the implementation of gradient descent with the following:**\n",
    ">\n",
    "> 1. Add the option for a diminishing stepsize, defined as:\n",
    ">\n",
    "> $$\\alpha_k = \\frac{\\alpha_0}{1+k}$$\n",
    ">\n",
    "> 2. Implement the update step using the variable stepsize:\n",
    ">\n",
    "> $$ {\\bf w} \\leftarrow {\\bf w} - \\alpha_k \\nabla J({\\bf w}). $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "quiz",
     "gradient_descent"
    ]
   },
   "outputs": [],
   "source": [
    "def gradient_descent(cost_fun, w_init, alpha, epochs, stepsize=None):\n",
    "    \"\"\"Find the point that minimizes the cost function.\n",
    "    \n",
    "    INPUTS:\n",
    "    cost_fun -- Cost function | callable\n",
    "    w_init   -- Initial point | numpy array\n",
    "    alpha    -- step-size     | scalar\n",
    "    epochs   -- n. iterations | integer\n",
    "    \n",
    "    OUTPUTS:\n",
    "    w       -- final point\n",
    "    history -- sequence of iterates w0, w1, ...\n",
    "    \"\"\"\n",
    "    \n",
    "    # automatic gradient\n",
    "    from autograd import grad\n",
    "    gradient = grad(cost_fun)\n",
    "    \n",
    "    # initialization\n",
    "    w = np.array(w_init).copy()\n",
    "    \n",
    "    # gradient descent\n",
    "    history = [w]   \n",
    "    for k in range(epochs):\n",
    "        \n",
    "        # update the stepsize\n",
    "        if stepsize == 'diminishing':\n",
    "            ak = None # YOUR CODE HERE\n",
    "        else:\n",
    "            ak = alpha\n",
    "        \n",
    "        # compute the next point\n",
    "        w = None # YOUR CODE HERE\n",
    "\n",
    "        # track the history\n",
    "        history.append(w)\n",
    "        \n",
    "    return w.squeeze(), np.stack(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"gradient_descent\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Linear regression\n",
    "\n",
    "The following quizzes will guide you through the solution of the optimization problem\n",
    "\n",
    "\n",
    "$$\n",
    "\\operatorname*{minimize}_{\\mathbf{w}\\in\\mathbb{R}^{N+1}}\\; \\frac{1}{P}\\sum_{p=1}^{P}\\Big(\\textsf{linear-model}\\left({\\bf x}^{(p)}; \\mathbf{w}\\right) - y^{(p)} \\Big)^2\n",
    "$$\n",
    "\n",
    "for a given set of input/output pairs $({\\bf x}^{(1)},y^{(1)}), \\dots, ({\\bf x}^{(P)},y^{(P)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 1.1\n",
    "\n",
    "> **Implement a function that evaluates the linear model on several input vectors $\\mathbf{x}^{(p)} \\in \\mathbb{R}^N$, namely** \n",
    ">\n",
    "> $$ \n",
    "\\textsf{linear-model}\\left({\\bf x}^{(p)}; \\mathbf{w} \\right) = w_{0} + w_1 x_1^{(p)} + \\dots + w_N x_N^{(p)} = \\mathbf{w}^\\top \\mathring{\\mathbf{x}}^{(p)}\n",
    "$$\n",
    ">\n",
    "> **for a given vector $\\mathbf{w}\\in\\mathbb{R}^{N+1}$.**\n",
    "\n",
    "> *Hint:* The function `linear_model()` takes two inputs:\n",
    "> - the weight vector $\\mathbf{w}\\in\\mathbb{R}^{N+1}$\n",
    "> - the matrix $X\\in\\mathbb{R}^{P\\times N}$ stacking the vectors ${\\bf x}^{(p)}\\in \\mathbb{R}^N$ along its rows:\n",
    ">\n",
    ">$$\n",
    "X = \\begin{bmatrix}\n",
    "\\_\\!\\_\\; {{\\bf x}^{(1)}}^\\top \\_\\!\\_ \\\\\n",
    "\\vdots \\\\\n",
    "\\_\\!\\_\\; {{\\bf x}^{(P)}}^\\top \\_\\!\\_ \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    ">\n",
    "> Consequently, the products ${\\bf w}^\\top\\mathring{\\mathbf{x}}^{(1)}, \\dots, {\\bf w}^\\top\\mathring{\\mathbf{x}}^{(P)}$ can be efficiently computed via a matrix-vector multiplication:\n",
    ">\n",
    ">$$ \n",
    "\\mathring{X} {\\bf w} \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "{\\mathring{\\bf x}^{(1)}}^\\top{\\bf w}\\\\\n",
    "\\vdots\\\\\n",
    "{\\mathring{\\bf x}^{(P)}}^\\top {\\bf w}\n",
    "\\end{bmatrix}.$$\n",
    ">\n",
    "> Note however that the shapes of input arrays `X` and `w` don't align for a matrix-vector product, as\n",
    "> - `w` is a vector of shape `(N+1,)`\n",
    "> - `X` is a matrix of shape `(P,N)`. \n",
    ">\n",
    "> To circumvent this issue, you can proceed as follows:\n",
    "> - multiply `X` with the sliced elements `w[1], ..., w[N]`\n",
    "> - add `w[0]` to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "quiz",
     "linear_model"
    ]
   },
   "outputs": [],
   "source": [
    "def linear_model(x, w):\n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "    x -- matrix of shape (P, N)\n",
    "    w -- vector of shape (N+1,)\n",
    "\n",
    "    OUTPUT:\n",
    "    y -- vector of shape (P,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute 'w[0] + w[1] * x[p,0] + ... + w[N] * x[p,N-1]' for each row of 'x'\n",
    "    y_hat = None # YOUR CODE HERE\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"linear_model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 1.2\n",
    "\n",
    "> **Implement the following cost function:**\n",
    ">\n",
    "> $$ \\big(\\forall {\\bf w}\\in\\mathbb{R}^{N+1}\\big)\\qquad J\\left(\\mathbf{w}\\right) = \\frac{1}{P}\\sum_{p=1}^{P}\\Big(\\textsf{model}\\left({\\bf x}^{(p)}; \\mathbf{w}\\right) - y^{(p)} \\Big)^2 $$\n",
    ">\n",
    "> **where the pairs $({\\bf x}^{(p)},y^{(p)})$ are given.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "quiz",
     "mse_cost"
    ]
   },
   "outputs": [],
   "source": [
    "def mse_cost(w, x, y, model):\n",
    "    \"\"\"     \n",
    "    Inputs:\n",
    "    w -- model weights\n",
    "    x -- model inputs\n",
    "    y -- expected model outputs\n",
    "    model -- function that evaluates a model on the inputs 'x' using the weights 'w'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Evaluate the model on 'x' using the weights 'w'\n",
    "    y_pred = None # YOUR CODE HERE\n",
    "\n",
    "    # Compute the mean squared error between 'y_hat' and 'y'\n",
    "    J = None # YOUR CODE HERE\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"mse_cost\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 1.3\n",
    "\n",
    "> **Use gradient descent to solve linear regression (i.e., to find the hyperplane that best fits the given points).** \n",
    "> - **Required:** The numerical error w.r.t. the exact solution must be less than $10^{-4}$.\n",
    "\n",
    "> *Hint:* You simply need to invoke the function `gradient_descent(...)` with the following parameters.\n",
    "> - `cost_fun` - **Cost function:** it's a Python callable defined by `def` or `lambda`.\n",
    "> - `w_init` - **Initialization:** it's a list that contains `N+1` float values.\n",
    "> - `alpha` - **Step-size:** it's a scalar value between $0$ and $1$. \n",
    "> - `epochs`  - **Number of iterations:** it's an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "quiz",
     "linear_regression"
    ]
   },
   "outputs": [],
   "source": [
    "# Random-generated data\n",
    "np.random.seed(42)\n",
    "P = 50\n",
    "N = 2\n",
    "X = np.random.randn(P, N)\n",
    "y = 1 - X[:,0] - X[:,1]\n",
    "\n",
    "# Define the cost function on the given 'X' and 'y'\n",
    "linear_mse_cost = None # YOUR CODE HERE\n",
    "\n",
    "# Set a random initialization\n",
    "w_init = None # YOUR CODE HERE\n",
    "\n",
    "# Set the step-size\n",
    "alpha = None # YOUR CODE HERE\n",
    "\n",
    "# Set the number of iterations\n",
    "epochs = None # YOUR CODE HERE\n",
    "\n",
    "# Perform the optimization by invoking 'gradient_descent()'\n",
    "... # YOUR CODE HERE\n",
    "\n",
    "# Visualize the history plot\n",
    "plt.plot([linear_mse_cost(wk) for wk in history])\n",
    "plt.title('History plot')\n",
    "plt.show()\n",
    "\n",
    "# Compare with the exact solution\n",
    "w_bar = [1, -1, -1]\n",
    "print('  Exact solution:', w_bar)\n",
    "print('Gradient descent:', w)\n",
    "print(' Numerical error: {:.0e}'.format(np.linalg.norm(w - w_bar)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"linear_regression\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Nonlinear regression\n",
    "\n",
    "The following quizzes will guide you through the solution of the optimization problem\n",
    "\n",
    "$$\n",
    "\\operatorname*{minimize}_{\\mathbf{w}\\in\\mathbb{R}^{N+1}}\\; \\frac{1}{P}\\sum_{p=1}^{P}\\Big(\\textsf{nonlinear-model}\\left(x^{(p)}; \\mathbf{w}\\right) - y^{(p)} \\Big)^2\n",
    "$$\n",
    "\n",
    "for a given set of input/output pairs $(x^{(1)},y^{(1)}), \\dots, (x^{(P)},y^{(P)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 2.1\n",
    "\n",
    "> **Implement a function that evaluates the hyperbolic tangent model on several input values $x^{(p)} \\in \\mathbb{R}$, namely** \n",
    ">\n",
    "> $$ \n",
    "\\textsf{tanh-model}\\left(x^{(p)};\\mathbf{w}\\right) = w_{0} + w_1 \\tanh\\left(w_2 + w_3 x^{(p)}\\right),\n",
    "$$\n",
    ">\n",
    "> **for a given vector $\\mathbf{w}=[w_0,w_1,w_2,w_3]\\in\\mathbb{R}^{4}$.**\n",
    "\n",
    "> *Hint:* [`np.tanh()`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.tanh.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "quiz",
     "tanh_model"
    ]
   },
   "outputs": [],
   "source": [
    "def tanh_model(x, w):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    x -- vector of shape (P,)\n",
    "    w -- vector of shape (4,)\n",
    "    \n",
    "    Returns:\n",
    "    y -- vector of shape (P,)\n",
    "    \"\"\"\n",
    "    \n",
    "    z = None # YOUR CODE HERE\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"tanh_model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 2.2\n",
    "\n",
    "> **Use gradient descent to solve the following optimization problem:** \n",
    ">\n",
    "> $$ \\operatorname*{minimize}_{\\mathbf{w}\\in\\mathbb{R}^{4}}\\; \\frac{1}{P}\\sum_{p=1}^{P}\\Big(\\textsf{tanh-model}\\left(x^{(p)}; \\mathbf{w}\\right) - y^{(p)} \\Big)^2, $$\n",
    ">\n",
    "> **where the pairs $(x^{(p)},y^{(p)})$ are given.**\n",
    ">\n",
    "> - **Required:** The cost function at the solution must be less than $0.09$.\n",
    "\n",
    "> *Hint:* For this problem, gradient descent needs a high number of iteration to converge. Make sure that the history plot gets \"perfectly\" flat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "quiz",
     "tanh_regression"
    ]
   },
   "outputs": [],
   "source": [
    "# Random-generated data\n",
    "np.random.seed(42)\n",
    "P = 50\n",
    "x = np.linspace(-1,1,P)\n",
    "y = 1 + np.tanh(-10*x + 0.5) + 0.3*np.random.randn(P)\n",
    "\n",
    "# Define the cost function on the given data 'x' and 'y'\n",
    "tanh_mse_cost = None # YOUR CODE HERE\n",
    "\n",
    "# Set a random initialization\n",
    "w_init = None # YOUR CODE HERE\n",
    "\n",
    "# Set the step-size\n",
    "alpha = None # YOUR CODE HERE\n",
    "\n",
    "# Set the number of iterations\n",
    "epochs = None # YOUR CODE HERE\n",
    "\n",
    "# Perform the optimization by invoking 'gradient_descent()'\n",
    "... # YOUR CODE HERE\n",
    "\n",
    "# Visualize the history plot\n",
    "plt.plot([tanh_mse_cost(wk) for wk in history])\n",
    "plt.title('History plot')\n",
    "plt.show()\n",
    "\n",
    "# Compare with the exact solution\n",
    "print('Solution found by gradient descent:', w)\n",
    "print('Cost function at the solution:', tanh_mse_cost(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"tanh_regression\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next figure plots the **tanh model** (red line) using the weights estimated by gradient descent. You should observe quite a good fit to the data (blues dots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.scatter(x, y, label='Data')\n",
    "plt.xlabel('x', fontsize='16')\n",
    "plt.ylabel('y', fontsize='16')\n",
    "\n",
    "t = np.linspace(-1,1,100)\n",
    "plt.plot(t, tanh_model(t, w), color='r', linewidth=3, label='Tanh model')\n",
    "\n",
    "plt.legend(fontsize='14')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 2.3\n",
    "\n",
    "> **Implement a function that evaluates the RELU model on several input values ${\\bf x} \\in \\mathbb{R}^N$, namely** \n",
    ">\n",
    "> $$ \n",
    "\\textsf{relu-model}\\left({\\bf x};\\mathbf{w}\\right) = \n",
    "%w_0^{(0)} + w_1^{(0)} \\max\\{0, w_0^{(1)} + w_1^{(1)} x\\} + \\dots + w_B^{(0)} \\max\\{0, w_0^{(B)} + w_1^{(B)} x\\},\n",
    "w_0^{(0)} + w_1^{(0)} \\max\\big\\{0, \\mathring{\\mathbf{x}}^\\top \\mathbf{w}^{(1)} \\big\\} + \\dots + w_B^{(0)} \\max\\big\\{0, \\mathring{\\mathbf{x}}^\\top \\mathbf{w}^{(B)} \\big\\},\n",
    "$$\n",
    ">\n",
    "> **for a given set of parameters $\\mathbf{w}=(\\mathbf{w}^{(0)},\\mathbf{w}^{(1)},\\dots,\\mathbf{w}^{(B)}) \\in\\mathbb{R}^{B+1}\\times\\mathbb{R}^{N+1}\\times\\dots\\times\\mathbb{R}^{N+1}$.**\n",
    "\n",
    "> *Hints:* \n",
    "> - [`np.maximum()`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.maximum.html)\n",
    "> - You may find useful the function `linear_model()` implemented in a previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "quiz",
     "relu_model"
    ]
   },
   "outputs": [],
   "source": [
    "def relu_model(x, w):\n",
    "    \"\"\"    \n",
    "    Inputs:\n",
    "    x -- matrix of shape (P, N)\n",
    "    w -- vector of shape ((N+2)*B+1,)\n",
    "    \n",
    "    Outputs:\n",
    "    y -- vector of shape (P,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure that 'x' is a matrix\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    \n",
    "    # Dimensions\n",
    "    N = x.shape[1]\n",
    "    B = (w.size - 1) // (N+2)\n",
    "    assert w.size == (N+2)*B+1\n",
    "            \n",
    "    # Select the first (N+1)*B weights and reshape them into a (N+1,B) matrix\n",
    "    w_relu = None # YOUR CODE HERE\n",
    "    \n",
    "    # Multiply 'x' by 'w_relu' --> You may use 'linear_model()'\n",
    "    z = None # YOUR CODE HERE\n",
    "    \n",
    "    # Clip the negative values in 'z' to zero --> You may use 'np.maximum()'\n",
    "    relu = None # YOUR CODE HERE\n",
    "    \n",
    "    # Select the last B+1 weights\n",
    "    w_linear = None # YOUR CODE HERE\n",
    "    \n",
    "    # Multiply 'relu' by 'w_linear' --> You may use 'linear_model()'\n",
    "    y = None # YOUR CODE HERE\n",
    "     \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"relu_model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 2.4\n",
    "\n",
    "> **Use gradient descent to solve the following optimization problem:** \n",
    ">\n",
    "> $$ \\operatorname*{minimize}_{\\mathbf{w}\\in\\mathbb{R}^{3B+1}}\\; \\frac{1}{P}\\sum_{p=1}^{P}\\Big(\\textsf{relu-model}\\left(x^{(p)}; \\mathbf{w}\\right) - y^{(p)} \\Big)^2, $$\n",
    ">\n",
    "> **where the pairs $(x^{(p)},y^{(p)})$ are given.**\n",
    ">\n",
    "> - **Required:** The cost function at the solution must be less than $0.05$.\n",
    "\n",
    "> *Hints:* \n",
    "> - For this problem, gradient descent needs a high number of iteration to converge. Make sure that the history plot gets \"perfectly\" flat.\n",
    "> - The choice of the step-size $\\alpha$ is critical for obtaining a satisfactory result. Try different values and select the one that leads to the smallest value of the cost function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "quiz",
     "relu_regression"
    ]
   },
   "outputs": [],
   "source": [
    "# Random-generated data\n",
    "np.random.seed(42)\n",
    "P = 50\n",
    "x = np.linspace(-2,2,P)\n",
    "y = np.sin(np.pi*x - np.pi) + 0.2*np.random.randn(P)\n",
    "\n",
    "# Define the cost function on the given data 'x' and 'y'\n",
    "relu_mse_cost = None # YOUR CODE HERE\n",
    "\n",
    "# Set a random initialization\n",
    "np.random.seed(5)\n",
    "B = 100\n",
    "w_init = None # YOUR CODE HERE\n",
    "\n",
    "# Set the step-size\n",
    "alpha = None # YOUR CODE HERE\n",
    "\n",
    "# Set the number of iterations\n",
    "epochs = None # YOUR CODE HERE\n",
    "\n",
    "# Perform the optimization by invoking 'gradient_descent()'\n",
    "... # YOUR CODE HERE\n",
    "\n",
    "# Visualize the history plot\n",
    "plt.plot([relu_mse_cost(wk) for wk in history])\n",
    "plt.title('History plot')\n",
    "plt.show()\n",
    "\n",
    "# Compare with the exact solution\n",
    "print('Cost function at the solution:', relu_mse_cost(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"relu_regression\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next figure plots the **relu model** (red line) using the weights estimated by gradient descent. You should observe quite a good fit to the data (blues dots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.scatter(x, y, label='Data')\n",
    "plt.xlabel('x', fontsize='16')\n",
    "plt.ylabel('y', fontsize='16')\n",
    "\n",
    "t = np.linspace(-2.1,2.1,100)\n",
    "plt.plot(t, relu_model(t, w), color='r', linewidth=3, label='Relu model')\n",
    "\n",
    "plt.legend(fontsize='14')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Polynomial regression\n",
    "\n",
    "The following quizzes will guide you through the solution of the optimization problem\n",
    "\n",
    "$$\n",
    "\\operatorname*{minimize}_{\\mathbf{w}\\in\\mathbb{R}^{N+1}}\\; \\frac{1}{P}\\sum_{p=1}^{P}\\Big(\\textsf{poly-model}\\left(x^{(p)}; \\mathbf{w}\\right) - y^{(p)} \\Big)^2\n",
    "$$\n",
    "\n",
    "for a given set of input-output pairs $(x^{(1)},y^{(1)}), \\dots, (x^{(P)},y^{(P)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 3.1\n",
    "\n",
    "> **For a given matrix $X$, compute the Lipschitz constant of the function $J\\left(\\mathbf{w}\\right) = \\frac{1}{P} \\| X\\mathbf{w}-\\mathbf{y} \\|^2$, which is given as:**\n",
    ">\n",
    "> $$ L = \\frac{2}{P} \\| X^\\top X \\|_2. $$\n",
    "\n",
    "> Hints:\n",
    "> - $\\|A\\|_2$ denotes the maximum eigenvalue of the symmetric matrix $A$.\n",
    "> - [`np.linalg.eigh()`](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.eigh.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "quiz",
     "lipschitz"
    ]
   },
   "outputs": [],
   "source": [
    "# Random-generated data\n",
    "np.random.seed(42)\n",
    "P = 50\n",
    "x = np.linspace(0,8,P)\n",
    "y = 0.2*x**2 + 0.1*x + np.random.randn(P)\n",
    "\n",
    "# Polynomial transformation\n",
    "d = 2\n",
    "X_poly = np.stack([x**i for i in range(d+1)], axis=1)\n",
    "\n",
    "# Lipschitz constant\n",
    "... # YOUR CODE HERE\n",
    "L = None # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"lipschitz\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 3.2\n",
    "\n",
    "> **Use gradient descent to solve the following optimization problem:** \n",
    ">\n",
    "> $$ \\operatorname*{minimize}_{\\mathbf{w}\\in\\mathbb{R}^{3}}\\; \\frac{1}{P} \\| X\\mathbf{w}-\\mathbf{y} \\|^2 $$\n",
    ">\n",
    "> **where $X\\in\\mathbb{R}^{P\\times 3}$ and $y\\in\\mathbb{R}^{3}$ are given.**\n",
    ">\n",
    "> - **Required:** The cost function at the solution must be less than $1$.\n",
    "\n",
    "> *Hints:* \n",
    "> - For this problem, gradient descent needs a high number of iteration to converge. Make sure that the history plot gets \"perfectly\" flat.\n",
    "> - Use the Lipschitz constant $L$ to set the step-size as $\\alpha = \\frac{1}{L}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "quiz",
     "poly_regression"
    ]
   },
   "outputs": [],
   "source": [
    "# Define the cost function on the given data 'X_poly' and 'y'\n",
    "poly_mse_cost = None # YOUR CODE HERE\n",
    "\n",
    "# Set a random initialization\n",
    "np.random.seed(42)\n",
    "w_init = None # YOUR CODE HERE\n",
    "\n",
    "# Set the step-size\n",
    "alpha = None # YOUR CODE HERE\n",
    "\n",
    "# Set the number of iterations\n",
    "epochs = None # YOUR CODE HERE\n",
    "\n",
    "# Perform the optimization by invoking 'gradient_descent()'\n",
    "... # YOUR CODE HERE\n",
    "\n",
    "# Visualize the history plot\n",
    "plt.plot([poly_mse_cost(wk) for wk in history])\n",
    "plt.title('History plot')\n",
    "plt.show()\n",
    "\n",
    "# Compare with the exact solution\n",
    "print('Cost function at the solution:', poly_mse_cost(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"poly_regression\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next figure plots the **poly model** (red line) using the weights estimated by gradient descent. You should observe quite a good fit to the data (blues dots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.scatter(x, y, label='Data')\n",
    "plt.xlabel('x', fontsize='16')\n",
    "plt.ylabel('y', fontsize='16')\n",
    "\n",
    "t = np.linspace(-1,9,100)\n",
    "X_test = np.stack([t**i for i in range(d+1)], axis=1)\n",
    "\n",
    "plt.plot(t, X_test@w, color='r', linewidth=3, label='Poly model')\n",
    "\n",
    "plt.legend(fontsize='14')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
