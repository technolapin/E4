{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize OK\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('lab04.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Gradient method\n",
    "\n",
    "This assignment is composed of 6 exercises. For each solved exercise, you get the points indicated below. You need to score at least **7 points** (out of 12) to pass the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "\n",
    " - Download a copy of this notebook from [Blackboard](https://esiee.blackboard.com/).\n",
    " \n",
    " \n",
    " - Run `jupyter notebook` on your computer, and open the `.ipynb` file that you just downloaded.\n",
    "\n",
    "\n",
    " - Solve the quizzes by filling in the cells with your solutions. \n",
    " \n",
    " \n",
    " - Check your answer by running the unit test provided at the end of each quiz.\n",
    " \n",
    " \n",
    " - **Get your work checked before leaving the lab, otherwise you won't get any credit for it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "| Exercise | Topic | Points |\n",
    "|----------|------|--------|\n",
    "| Quiz 1 | Automatic differentiation | 1 |\n",
    "| Quiz 2 | Gradient descent | 2 |\n",
    "| Quiz 3 | Convex function of one variable | 2 |\n",
    "| Quiz 4 | Convex function of multiple variables | 2 |\n",
    "| Quiz 5 | Nonconvex function of one variable | 2 |\n",
    "| Quiz 6 | Nonconvex function of multiple variables | 3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages\n",
    "\n",
    "For this assignment, you need to import the following packages.\n",
    "- [**Numpy**](www.numpy.org) - The library for scientific computing in Python.\n",
    "- [**Matplotlib**](http://matplotlib.org) - The library for plotting graphs in Python.\n",
    "- [**Autograd**](https://github.com/HIPS/autograd) - The library for automatic differentiation of Numpy code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 1. Automatic differentiation\n",
    "\n",
    "> **Implement the following function, and then compute its gradient with autograd:** \n",
    ">\n",
    "> $$ (\\forall {\\bf w}=[w_0,w_1,\\dots,w_N]^\\top\\in \\mathbb{R}^{N+1})\\qquad f({\\bf w}) = w_0 + a_1 w_1 + \\dots a_N w_N. $$\n",
    ">\n",
    "> **where the vactor ${\\bf a} = [a_1,\\dots,a_N]^\\top \\in \\mathbb{R}^{N}$ is given.**\n",
    "\n",
    "> *Hints:* Note that the vectors ${\\bf w}$ and ${\\bf a}$ don't have the same size:\n",
    "> - `w` - vector of shape `(N+1,)`\n",
    "> - `a` - vector of shape `(N,)`. \n",
    ">\n",
    "> To get around this, you can proceed as follows:\n",
    "> - Slice `w` so as to extract the elements `w[1], ..., w[N]`.\n",
    "> - Compute the scalar product between `a` and the above elements.\n",
    "> - Add `w[0]` to the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "quiz",
     "autograd"
    ]
   },
   "outputs": [],
   "source": [
    "function = None # YOUR CODE HERE\n",
    "gradient = None # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"autograd\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 2. Gradient descent\n",
    "\n",
    "> **Complete the implementation of gradient descent, where you need to code the update step:** \n",
    ">\n",
    "> $$ {\\bf w} \\leftarrow {\\bf w} - \\alpha\\nabla J({\\bf w}). $$\n",
    "\n",
    "> *Remark.* The values returned by `gradient_descent()` are\n",
    ">  - the final point ${\\bf w}_K$,\n",
    ">  - the entire sequence of iterates ${\\bf w}_0, {\\bf w}_1, \\dots, {\\bf w}_K$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "quiz",
     "gradient_descent"
    ]
   },
   "outputs": [],
   "source": [
    "def gradient_descent(cost_fun, w_init, alpha, epochs):\n",
    "    \"\"\"Find the point that minimizes the cost function.\n",
    "    \n",
    "    INPUTS:\n",
    "    cost_fun -- Cost function | callable\n",
    "    w_init   -- Initial point | numpy array\n",
    "    alpha    -- step-size     | scalar\n",
    "    epochs   -- n. iterations | integer\n",
    "    \n",
    "    OUTPUTS:\n",
    "    w       -- final point\n",
    "    history -- sequence of iterates w0, w1, ...\n",
    "    \"\"\"\n",
    "    \n",
    "    # automatic gradient\n",
    "    from autograd import grad\n",
    "    gradient = grad(cost_fun)\n",
    "    \n",
    "    # initialization\n",
    "    w = np.array(w_init).copy()\n",
    "    \n",
    "    # gradient descent\n",
    "    history = [w]   \n",
    "    for k in range(epochs):\n",
    "        \n",
    "        # compute the next point\n",
    "        w = None # YOUR CODE HERE\n",
    "\n",
    "        # track the history\n",
    "        history.append(w)\n",
    "        \n",
    "    return w.squeeze(), np.stack(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"gradient_descent\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below displays the set of points ${\\bf w}_0, {\\bf w}_1,\\dots, {\\bf w}_K$ generated by gradient descent for the line fitting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_points(N=20, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    x = np.random.rand(N)\n",
    "    y = 0.5*x + 2 + 0.05 * np.random.randn(N)\n",
    "    return x, y\n",
    "\n",
    "def show_history(history, x, y, lines=True):\n",
    "    a_values, b_values = np.meshgrid(np.arange(-1,2.5,0.05), np.arange(1,2.7,0.05))\n",
    "    J_values = np.sum((a_values.flatten()[:,None] * x + b_values.flatten()[:,None] - y)**2, axis=1)\n",
    "    J_values = J_values.reshape(a_values.shape)\n",
    "    fig = plt.figure(figsize=(7,5))\n",
    "    draw_contours(a_values, b_values, J_values)\n",
    "    draw_points(history, lines)\n",
    "    plt.show()\n",
    "    \n",
    "def draw_contours(X, Y, Z):\n",
    "    from matplotlib.colors import LogNorm\n",
    "    cs = plt.contour(X, Y, Z, levels=np.logspace(-1.5,2,9), norm=LogNorm(), alpha=.4, colors = 'k')\n",
    "    plt.clabel(cs, inline=1, fontsize=10)\n",
    "    plt.contourf(X, Y, Z, levels=np.logspace(-1.5,2,9), norm=LogNorm(), cmap='jet', alpha=.4)\n",
    "    \n",
    "def draw_points(history, lines=True):\n",
    "    colorspec = make_colorspec(history)\n",
    "    plt.scatter(history[:,0], history[:,1], color=colorspec, s=80, edgecolor='k', alpha=0.9, zorder=3)\n",
    "    if lines:\n",
    "        plt.plot(history[:,0], history[:,1], color='k', alpha=0.7, linewidth=2, zorder=2)\n",
    "    \n",
    "def make_colorspec(w_hist):\n",
    "    s = np.linspace(0,1,len(w_hist[:round(len(w_hist)/2)]))\n",
    "    s.shape = (len(s),1)\n",
    "    t = np.ones(len(w_hist[round(len(w_hist)/2):]))\n",
    "    t.shape = (len(t),1)\n",
    "    s = np.vstack((s,t))\n",
    "    colorspec = []\n",
    "    colorspec = np.concatenate((s,np.flipud(s)),1)\n",
    "    colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n",
    "    return colorspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_points()\n",
    "\n",
    "cost = lambda w: np.sum((w[0]*x + w[1] - y)**2)\n",
    "\n",
    "w, history = gradient_descent(cost, w_init=[1,1.05], alpha=0.01, epochs=100)\n",
    "\n",
    "show_history(history, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 3. Convex function of one variable\n",
    "\n",
    "> **1. Implement the following cost function with NumPy operations supported by autograd:**\n",
    ">\n",
    "> $$ (\\forall w\\in\\mathbb{R})\\qquad J(w) = \\frac{1}{50}\\Big( w^4 + w^2 + 10w\\Big). $$\n",
    "\n",
    "> **2. Compute the (global) minimum with the analytical expression:**\n",
    ">\n",
    "> $$ \\bar{w} = \\sqrt[3]{\\frac{\\sqrt{2031}-45}{36}} - \\sqrt[3]{\\frac{1}{6\\left(\\sqrt{2031}-45\\right)}}. $$\n",
    "\n",
    "> **3. Find the minimum point with gradient descent, and check that the numerical error w.r.t. the exact solution $\\bar{w}$ is less than $10^{-5}$.**\n",
    "\n",
    "> *Hints:* \n",
    ">  - You may find useful the functions [`np.sqrt()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sqrt.html) and [`np.cbrt()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.cbrt.html)\n",
    ">  - Evaluate the error through the absolute difference $|w - \\bar{w}|$\n",
    ">  - To reduce the numerical error, increase the number of iterations in gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "quiz",
     "convex_one"
    ]
   },
   "outputs": [],
   "source": [
    "# Answer 1. Cost function\n",
    "cost_fun = None # YOUR CODE HERE\n",
    "\n",
    "# Answer 2. Analytical solution\n",
    "w_bar = None # YOUR CODE HERE\n",
    "\n",
    "# Answer 3a. Optimization\n",
    "... # YOUR CODE HERE\n",
    "\n",
    "# Answer 3b. Numerical error\n",
    "error = None # YOUR CODE HERE\n",
    "\n",
    "print('  Exact solution:', w_bar)\n",
    "print('Gradient descent:', w)\n",
    "print(' Numerical error: {:.0e}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"convex_one\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 4. Convex function of multiple variables\n",
    "\n",
    "> **1. Implement the following cost function with NumPy operations supported by autograd:**\n",
    ">\n",
    "> $$ \\big(\\forall {\\bf w}\\in\\mathbb{R}^N\\big)\\qquad J({\\bf w}) = \\frac{1}{2}\\|A{\\bf w}-{\\bf b}\\|^2 $$\n",
    ">\n",
    "> **where the matrix $A \\in \\mathbb{R}^{P\\times N}$ and the vector ${\\bf b} \\in \\mathbb{R}^{P}$ are given.**\n",
    "\n",
    "> **2. Compute the (global) minimum with the analytical expression:**\n",
    ">\n",
    "> $$ {\\bf\\bar{w}} = \\big(A^\\top A\\big)^{-1} A^\\top b $$\n",
    ">\n",
    "> **using $A=\\begin{bmatrix} 1 & 2 \\\\ 3 & 2 \\end{bmatrix}$ and  ${\\bf b} = \\begin{bmatrix}1 \\\\ -2 \\end{bmatrix}$.**\n",
    "\n",
    "> **3. Find the minimum point with gradient descent, and check that the numerical error w.r.t. the exact solution $\\bar{\\bf w}$ is less than $10^{-5}$.**\n",
    "\n",
    "> *Hints:* \n",
    ">  - Recall the basic operations of linear algebra in NumPy:\n",
    "      - The operator [`@`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html#numpy.matmul) performs a matrix multiplation;\n",
    "      - The function [`np.linalg.inv`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html) performs a matrix inversion;\n",
    "      - The attribute [`.T`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.ndarray.T.html) performs a matrix transpose.\n",
    "      - The function [`np.linalg.norm`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html) computes the norm $\\|\\cdot\\|$ of a vector/matrix.\n",
    ">  - Evaluate the error through the Euclidean distance $\\|{\\bf w} - {\\bf\\bar{w}}\\|$.\n",
    ">  - To reduce the numerical error, increase the number of iterations in gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "quiz",
     "convex_multi"
    ]
   },
   "outputs": [],
   "source": [
    "# Answer 1. Cost function\n",
    "def quadratic_function(w, A, b):\n",
    "    w = np.array(w)\n",
    "    J = None # YOUR CODE HERE\n",
    "    return J\n",
    "\n",
    "# Answer 2. Analytical solution\n",
    "A = np.array([[1.,2],[3,2]])\n",
    "b = np.array([1.,-2])\n",
    "w_bar = None # YOUR CODE HERE\n",
    "\n",
    "# Answer 3a. Optimization\n",
    "cost_fun = lambda w: quadratic_function(w, A, b)\n",
    "... # YOUR CODE HERE\n",
    "\n",
    "# Answer 3b. Numerical error\n",
    "error = None # YOUR CODE HERE\n",
    "\n",
    "print('  Exact solution:', w_bar)\n",
    "print('Gradient descent:', w)\n",
    "print(' Numerical error: {:.0e}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"convex_multi\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 5. Nonconvex function of one variable\n",
    "\n",
    "> **1. Implement the following cost function with NumPy operations supported by autograd:**\n",
    ">\n",
    "> $$ (\\forall w\\in\\mathbb{R})\\qquad J(w) = \\cos(2w+1) + 0.2 w^2. $$\n",
    "\n",
    "> **2. Find the global minimum with gradient descent, and check that the numerical error w.r.t. the exact solution $\\bar{w}\\approx0.972883$ is small.**\n",
    "\n",
    "> *Hints:* \n",
    ">  - You may find useful the function [`np.cos()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.cos.html)\n",
    ">  - You should try out different initializations, until gradient descent converges to the global minimum.\n",
    ">  - Evaluate the error through the absolute difference $|w - \\bar{w}|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "quiz",
     "nonconvex_one"
    ]
   },
   "outputs": [],
   "source": [
    "# Answer 1. Cost function\n",
    "cost_fun = None # YOUR CODE HERE\n",
    "\n",
    "# Answer 2a. Optimization\n",
    "... # YOUR CODE HERE\n",
    "\n",
    "# Answer 2b. Numerical error\n",
    "w_bar = 0.972883\n",
    "error = None # YOUR CODE HERE\n",
    "\n",
    "print('  Exact solution:', 1)\n",
    "print('Gradient descent:', w)\n",
    "print(' Numerical error: {:.0e}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"nonconvex_one\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 6. Nonconvex function of mutiple variables\n",
    "\n",
    "> **1. Implement the following cost function with NumPy operations supported by autograd:**\n",
    ">\n",
    "> $$ \\big(\\forall (\\mathbf{c}_1, \\dots, \\mathbf{c}_K)\\in(\\mathbb{R}^N)^K\\big)\\qquad J(\\mathbf{c}_1, \\dots, \\mathbf{c}_K) = \\sum_{p=1}^P\\left( \\min_{k\\in\\{1,\\dots,K\\}}\\; \\left\\Vert \\mathbf{x}_p - \\mathbf{c}_k \\right\\Vert\\right), $$\n",
    ">\n",
    "> **where the vectors ${\\bf x}_p \\in \\mathbb{R}^{N}$ are given.**\n",
    "\n",
    "> *Hint:* You mau find useful the function [`scipy.spatial.distance_matrix`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "quiz",
     "kmeans_cost"
    ]
   },
   "outputs": [],
   "source": [
    "def kmeans_cost(c, X):\n",
    "    \"\"\"\n",
    "    c -- centroids   | matrix of shape (K, N)\n",
    "    X -- data points | matrix of shape (P, N)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the squared distances ||x_p - c_k|| for all p and k. Stock them in a matrix of shape (P, K)\n",
    "    dist = None # YOUR CODE HERE\n",
    "        \n",
    "    # Compute the minimum 'min_k ||x_p - c_k||'. Perform the operation along the right axis!\n",
    "    mins = None # YOUR CODE HERE\n",
    "    \n",
    "    # Sum everything to get the cost \n",
    "    cost = None # YOUR CODE HERE\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"kmeans_cost\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **2. Find the global minimum of $J(\\mathbf{c}_1, \\dots, \\mathbf{c}_K)$ with gradient descent.**\n",
    "\n",
    "> *Hint:* It is important to choose well the initial centroids `c0`. Spread them around, instead of putting them in the same spot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "quiz",
     "kmeans_optimization"
    ]
   },
   "outputs": [],
   "source": [
    "# Data points\n",
    "X, y = datasets.make_blobs(n_samples=30, random_state=42)\n",
    "\n",
    "# Cost function\n",
    "cost_fun = None # YOUR CODE HERE\n",
    "\n",
    "# Initialization\n",
    "np.random.seed(0)\n",
    "c0 = np.zeros((3,2))\n",
    "c0 = None # YOUR CODE HERE\n",
    "\n",
    "# Optimization\n",
    "c = c0\n",
    "... # YOUR CODE HERE\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[:,0], X[:,1], color='lightgray', edgecolor='k', label='Data points')\n",
    "plt.scatter(c[:,0], c[:,1], color=[(1,0,0.4), (0, 0.4, 1), (0, 1, 0.5)], s = 400, \n",
    "            edgecolor ='k', linewidth = 2, marker=(5, 1), zorder = 3, label='Centroids')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"kmeans_optimization\");"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
